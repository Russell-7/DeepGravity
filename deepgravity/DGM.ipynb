{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "env : yh_Diffuser\n",
    "'''\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pickle\n",
    "from torch import nn\n",
    "import glob\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from scipy.special import kl_div\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from math import pi\n",
    "# from metrics import CPC, JSD_ODflow\n",
    "from model import DeepGravity\n",
    "mseloss = nn.MSELoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    output_dir = \"/mnt/data/chunhuachen/UMWIFI_TOOL/temp/\"\n",
    "config = TrainingConfig()\n",
    "test_dir = os.path.join(config.output_dir, \"split\")\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class GravityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, labels):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        # self.days_per_month = [np.load(path, mmap_mode='r').shape[0] for path in paths]\n",
    "        # self.days_per_month = [31,28,31,30,31,30,31,31,30,31,30,31,\n",
    "        #                        31,28,31,30,31,30,31,31,30,31,30,31,\n",
    "        #                        28,31,30,31,30,31,31,30,31,30,31,]\n",
    "        self.days_per_month = [31]\n",
    "        self.max = 157\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(day * 54 * 53 * 144 for day in self.days_per_month)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 计算.npy文件的索引\n",
    "        path_idx = idx // (144*54*53)\n",
    "        \n",
    "        data = np.load(self.paths[path_idx], mmap_mode='r')\n",
    "        # 计算OD pair和时间段的索引\n",
    "        time_frame_idx = idx % 144\n",
    "        od_idx = idx % (54*53)\n",
    "        i = od_idx // 53\n",
    "        j = od_idx % 53\n",
    "        if j >= i:\n",
    "            j += 1\n",
    "        od_flow = data[time_frame_idx, i, j]/self.max\n",
    "\n",
    "        time_idx = idx % 144\n",
    "        # time_idx = idx // (53*54)\n",
    "\n",
    "        # 获取OD pair的特征\n",
    "        # features = data[idx // (54*53*144), i, j]\n",
    "        # 计算流量的占比\n",
    "        # fraction = features / np.sum(data[idx // (54*53*144), i, :], axis=-1)\n",
    "        # odflow_sum = np.sum(data[time_frame_idx, i, :])\n",
    "        # od_sum = 1 if odflow_sum <= 0 else odflow_sum\n",
    "        # od_sum = 1 if np.sum(data[idx // (54*53*144), i, :]) <= 0 else np.sum(data[idx // (54*53*144), i, :])\n",
    "        # fraction = features / od_sum\n",
    "        # 获取标签并添加新的元素\n",
    "        label = self.labels[(path_idx + 31+28) * 144 + time_idx].copy()\n",
    "        label.extend([171+i, 226+j])\n",
    "        # label = torch.stack(label)\n",
    "        # return features, fraction, label\n",
    "        return od_flow, label\n",
    "    \n",
    "# paths = []\n",
    "# for i in range(1, 5):  # 对于1到4位的数字\n",
    "#     pattern = f'/mnt/data/chunhuachen/UMWIFI_TOOL/data/UMWIFI_FINAL_SET/UMWIFI_202?/TRACE/month_??_trace_build_low10_up500_devices{\"[0-9]\"*i}_UserCentric_diagonal_insert_flow_outnode.npy'\n",
    "#     paths.extend(glob.glob(pattern))\n",
    "# paths.sort()\n",
    "# data = [np.load(npy_dir, mmap_mode='r') for npy_dir in paths]\n",
    "pattern = test_dir + \"/month_02_day_??.npy\" \n",
    "paths = glob.glob(pattern)\n",
    "paths.sort()\n",
    "with open('/home/dingqiyang/chunhuachen/umwifi/diffusers/jupyter_test/label_21_22_23_year.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "dataset = GravityDataset(paths,labels)\n",
    "batchsize = 1024\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.955130968184676e-06: 100%|██████████| 12477/12477 [32:00<00:00,  6.50it/s] \n",
      "Epoch 2, Loss: 1.5150536455621477e-05: 100%|██████████| 12477/12477 [32:39<00:00,  6.37it/s]\n",
      "Epoch 3, Loss: 1.1354882190062199e-05: 100%|██████████| 12477/12477 [33:14<00:00,  6.26it/s]\n",
      "Epoch 4, Loss: 6.5063932197517715e-06: 100%|██████████| 12477/12477 [33:31<00:00,  6.20it/s]\n",
      "Epoch 5, Loss: 1.090844762074994e-05: 100%|██████████| 12477/12477 [32:41<00:00,  6.36it/s] \n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "config = batchsize\n",
    "model = DeepGravity(config).to(device)\n",
    "# # 定义损失函数\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# # 定义优化器\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=5e-6, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# 定义训练周期数\n",
    "epochs = 5\n",
    "from tqdm import tqdm\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for i, (od_flow, label) in enumerate(pbar):\n",
    "        od_flow = od_flow.float()\n",
    "        od_flow = od_flow.to(device)\n",
    "        label = torch.stack(label,axis=1).to(device)\n",
    "        outputs = model(label)\n",
    "        # print(outputs)\n",
    "        loss = criterion(outputs.squeeze(), od_flow)\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./trained_model/DGM_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepGravity(\n",
       "  (embed): Embedding(280, 128)\n",
       "  (linear_in): Linear(in_features=1152, out_features=64, bias=True)\n",
       "  (linears): ModuleList(\n",
       "    (0-4): 5 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (linear_out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_21_22_23_year_date.pkl是有误版本，label_21_22_23_year_date_update.pkl是更新版本\n",
    "with open('/home/dingqiyang/chunhuachen/umwifi/diffusers/jupyter_test/label_21_22_23_year_date_update.pkl', 'rb') as f:\n",
    "    picked_labels_index = pickle.load(f)\n",
    "tuple_of_lists = [tuple(x[1:]) for x in picked_labels_index]\n",
    "indices_dict = {}\n",
    "for i, sublist in enumerate(tuple_of_lists):\n",
    "    sublist_tuple = tuple(sublist)\n",
    "    if sublist_tuple not in indices_dict:\n",
    "        indices_dict[sublist_tuple] = [i]\n",
    "    else:\n",
    "        indices_dict[sublist_tuple].append(i)\n",
    "#monday ==> Mon, no day-off, no weather, offline\n",
    "regular_index = indices_dict[(144, 151,156,160,165, 168)] \n",
    "#saturday ==> Sat, no day-off, no weather, offline\n",
    "saturday_index = indices_dict[(149, 151,156,160,165, 168)] \n",
    "#hybrid ==> Fri, no day-off, no weather, HYBRID\n",
    "hybrid_index = indices_dict[(148, 151,156,160,165, 169)] \n",
    "#recess ==> Thurs, RECESS, no weather, offline\n",
    "recess_index = indices_dict[(147, 155,156,160,165, 168)] \n",
    "#typhoon ==> Sunday,no vacation, NO.8!!!, offline\n",
    "tropical_8_index = indices_dict[(150, 151, 159, 164, 167, 168)] \n",
    "#black ==> Tues, examination, no typhoon, BLACK, offline\n",
    "black_rain_index = indices_dict[(145, 152, 156, 163, 166, 168)]\n",
    "#suspended ==> Sunday,no day-off,no typhoon,no raninstorm, THUNDERSTORM,SUSPENDED\n",
    "suspended_index = indices_dict[(150, 151, 156, 160, 166, 171)]\n",
    "#examination ==> Monday, EXAMINATION, THUNDER,normal [143,150]\n",
    "examination_thunder_index = indices_dict[(144, 152, 156, 160, 166, 168)]\n",
    "#hybrid holiday ==> Wed,HOLIDAY,HYBRID [264]\n",
    "hybrid_holiday_index = indices_dict[(146, 153, 156, 160, 165, 169)]\n",
    "index_name = ['regular','saturday','hybrid', 'recess','suspended','tropical_8', 'black',\"examination_thunder\",\"hybrid_holiday\"]\n",
    "index_set_all = [regular_index,saturday_index,hybrid_index,recess_index,suspended_index,tropical_8_index,black_rain_index,examination_thunder_index,hybrid_holiday_index]\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    output_dir = \"/home/dingqiyang/chunhuachen/umwifi/DGM\"\n",
    "config = TrainingConfig()\n",
    "eval_dir = os.path.join(config.output_dir, \"evaluation\")\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/dingqiyang/.cache/huggingface/datasets/generator/default-cb3aa627cd75d403/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "for i in range(1, 5):  # 对于1到4位的数字\n",
    "    pattern = f'/mnt/data/chunhuachen/UMWIFI_TOOL/data/UMWIFI_FINAL_SET/UMWIFI_202?/TRACE/month_??_trace_build_low10_up500_devices{\"[0-9]\"*i}_UserCentric_diagonal_insert_flow_outnode.npy'\n",
    "    paths.extend(glob.glob(pattern))\n",
    "paths.sort()\n",
    "data = [np.load(npy_dir, mmap_mode='r') for npy_dir in paths]\n",
    "\n",
    "with open('/home/dingqiyang/chunhuachen/umwifi/diffusers/jupyter_test/label_21_22_23_year.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "def my_gen():\n",
    "    label_count = 0\n",
    "    for npy in data:\n",
    "        # img = Image.fromarray(data[i][0].astype(np.uint8),\"L\")\n",
    "        for i in range(npy.shape[0]):        \n",
    "            img = npy[i].transpose(2,0,1)\n",
    "            label = labels[label_count]\n",
    "            label_count += 1\n",
    "            yield {\"image\": img,\"label\":label}\n",
    "dataset_eval = Dataset.from_generator(my_gen)\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(torch.from_numpy(np.array(image))).float() for image in examples[\"image\"]]\n",
    "    labels = [torch.tensor(label) for label in examples[\"label\"]]\n",
    "    return {\"images\": images,\"labels\":labels}\n",
    "\n",
    "dataset_eval.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(model, index_set, filename):\n",
    "    # 初始化一个四维张量来存储结果\n",
    "    time_slices=144\n",
    "    matrix_size=54\n",
    "    result = torch.zeros((time_slices, 1, matrix_size, matrix_size))\n",
    "    # 遍历每个时间切片\n",
    "    for t in range(time_slices):\n",
    "        # 创建一个空的输入列表\n",
    "        inputs = []\n",
    "        # 创建一个空的索引列表\n",
    "        indices = []\n",
    "        input = dataset_eval[index_set[0]*144+t]['labels']\n",
    "        # 遍历矩阵的每个元素\n",
    "        for i in range(matrix_size):\n",
    "            for j in range(matrix_size):\n",
    "                # 对角线元素不生成，全为0\n",
    "                if i != j:\n",
    "                    # 创建一个新的输入，包含原始输入和当前的行列索引\n",
    "                    new_input = torch.cat((input, torch.tensor([i+172, j+226]))).to(device)\n",
    "                    # 将新的输入添加到输入列表\n",
    "                    inputs.append(new_input)\n",
    "                    # 将当前的行列索引添加到索引列表\n",
    "                    indices.append((i, j))\n",
    "\n",
    "        # 将输入列表转换为张量\n",
    "        inputs = torch.stack(inputs)\n",
    "\n",
    "        # 使用模型生成新的值，禁用梯度计算\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        # 将输出的值匹配到结果矩阵的非对角线元素\n",
    "        for k, (i, j) in enumerate(indices):\n",
    "            result[t, 0, i, j] = outputs[k]\n",
    "\n",
    "    result = result*157\n",
    "    torch.save(result, eval_dir + f'/{filename}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular\n",
      "saturday\n",
      "hybrid\n",
      "recess\n",
      "suspended\n",
      "tropical_8\n",
      "black\n",
      "examination_thunder\n",
      "hybrid_holiday\n"
     ]
    }
   ],
   "source": [
    "index_name = ['regular','saturday','hybrid', 'recess','suspended','tropical_8', 'black',\"examination_thunder\",\"hybrid_holiday\"]\n",
    "index_set_all = [regular_index,saturday_index,hybrid_index,recess_index,suspended_index,tropical_8_index,black_rain_index,examination_thunder_index,hybrid_holiday_index]\n",
    "for i, index_set in enumerate(index_set_all):\n",
    "    generate_matrix(model, index_set, index_name[i])\n",
    "    print(index_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from metrics import CPC, JSD_ODflow\n",
    "mseloss = nn.MSELoss()\n",
    "def evaluate(original,images):\n",
    "    CPC_list_diag = []\n",
    "    JSD_list_diag = []\n",
    "    RMSE_list_diag = []\n",
    "    NRMSE_list_diag = [] \n",
    "    CPC_list_OD = []\n",
    "    JSD_list_OD = []\n",
    "    RMSE_list_OD = []\n",
    "    NRMSE_list_OD = [] \n",
    "    for j in range(images.shape[0]):\n",
    "        diagonal_gen = (images[j,0,0:53,0:53].diag()*1786)\n",
    "        diagonal_ori = (original[j,0,0:53,0:53].diag()*1786)\n",
    "        CPC_diag = CPC(diagonal_gen,diagonal_ori)\n",
    "        JSD_diag = JSD_ODflow(diagonal_gen,diagonal_ori)\n",
    "        rmse_diag = torch.sqrt(mseloss(diagonal_gen,diagonal_ori))\n",
    "        nrmse_diag = rmse_diag/1786\n",
    "        CPC_list_diag.append(CPC_diag)\n",
    "        JSD_list_diag.append(JSD_diag)\n",
    "        RMSE_list_diag.append(rmse_diag)\n",
    "        NRMSE_list_diag.append(nrmse_diag)\n",
    "        # 计算其余值的CPC，JSD和RMSE\n",
    "        OD_gen = (images[j,0,:,:].clone())#失误，在生成的时候就×过了\n",
    "        OD_ori = (original[j,0,:,:].clone())*530\n",
    "        OD_gen.fill_diagonal_(0)\n",
    "        OD_ori.fill_diagonal_(0)\n",
    "        CPC_OD = CPC(OD_gen, OD_ori)\n",
    "        JSD_OD = JSD_ODflow(OD_gen, OD_ori)\n",
    "        rmse_OD = torch.sqrt(mseloss(OD_gen, OD_ori))\n",
    "        nrmse_OD = rmse_OD/530\n",
    "        CPC_list_OD.append(CPC_OD)\n",
    "        JSD_list_OD.append(JSD_OD)\n",
    "        RMSE_list_OD.append(rmse_OD)\n",
    "        NRMSE_list_OD.append(nrmse_OD)\n",
    "# print(len(CPC_list_diag))\n",
    "    return CPC_list_diag, JSD_list_diag, RMSE_list_diag, NRMSE_list_diag, CPC_list_OD, JSD_list_OD, RMSE_list_OD, NRMSE_list_OD\n",
    "\n",
    "def index_set_evaluate(index_set, filename):\n",
    "    metrics = [\"cpc_flow\", \"jsd_flow\", \"rmse_flow\", \"nrmse_flow\", \"cpc_od\", \"jsd_od\", \"rmse_od\", \"nrmse_od\"]\n",
    "    results = {metric + \"_all\": [] for metric in metrics}\n",
    "\n",
    "\n",
    "    images = torch.load(eval_dir + f'/{filename}.pth')\n",
    "\n",
    "\n",
    "    for i, index in enumerate(index_set):\n",
    "        day = torch.stack(dataset_eval[index * 144 : index * 144 + 144]['images'])\n",
    "        day = day * 0.5 + 0.5\n",
    "        metrics_values = evaluate(day, images.cpu())\n",
    "        for metric, value in zip(metrics, metrics_values):\n",
    "            results[metric + \"_all\"].append(value)\n",
    "\n",
    "    print(filename, \"Index length\", len(index_set))\n",
    "    results_avg = {}\n",
    "    for metric in metrics:\n",
    "        # avg = np.mean(results[metric + \"_all\"], axis=0).mean()\n",
    "        avg = np.mean(results[metric + \"_all\"])\n",
    "        # print(metric + \":\", avg)\n",
    "        results_avg[metric + \"_avg\"] = avg\n",
    "\n",
    "    # print(filename, \"Index length\", len(index_set), \", \".join(map(str, [results_avg[metric + \"_avg\"] for metric in metrics])))\n",
    "    return results_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular Index length 31\n",
      "saturday Index length 36\n",
      "hybrid Index length 30\n",
      "recess Index length 18\n",
      "suspended Index length 1\n",
      "tropical_8 Index length 1\n",
      "black Index length 1\n",
      "examination_thunder Index length 2\n",
      "hybrid_holiday Index length 1\n"
     ]
    }
   ],
   "source": [
    "pipeline = []\n",
    "metrics = [\"cpc_flow\", \"jsd_flow\", \"rmse_flow\", \"nrmse_flow\", \"cpc_od\", \"jsd_od\", \"rmse_od\", \"nrmse_od\"]\n",
    "all_results = {metric + \"_all_set_avg\": [] for metric in metrics}\n",
    "with open(f'./evaluation/results_DGM.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([metric + \"_avg\" for metric in metrics])  # 写入表头\n",
    "    for i, index_set in enumerate(index_set_all):\n",
    "            results = index_set_evaluate(index_set, index_name[i])\n",
    "            writer.writerow([results[metric + \"_avg\"] for metric in metrics])\n",
    "            writer.writerow([])\n",
    "            writer.writerow([])\n",
    "            writer.writerow([])\n",
    "            writer.writerow([])\n",
    "            writer.writerow([])\n",
    "            writer.writerow([])\n",
    "            for metric in metrics:\n",
    "                all_results[metric + \"_all_set_avg\"].append(results[metric+'_avg'])\n",
    "    all_results_avg = {metric: np.mean(values) for metric, values in all_results.items()}\n",
    "    writer.writerow([all_results_avg[metric + \"_all_set_avg\"] for metric in metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yh_Diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
